#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿ƒç†å­¦è¯æ¡HTMLæ–‡ä»¶çº¯ä¸­æ–‡å†…å®¹æå–è„šæœ¬
ä»5ä¸ªå¿ƒç†å­¦æ¦‚å¿µçš„HTMLæ–‡ä»¶ä¸­æå–çº¯ä¸­æ–‡æ–‡æœ¬å†…å®¹
"""

import os
import re
from bs4 import BeautifulSoup

def is_chinese_char(char):
    """åˆ¤æ–­å­—ç¬¦æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦"""
    return '\u4e00' <= char <= '\u9fff'

def is_pure_chinese_text(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºçº¯ä¸­æ–‡å†…å®¹"""
    if not text or len(text.strip()) < 3:
        return False
    
    text = text.strip()
    
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
    chinese_chars = sum(1 for char in text if is_chinese_char(char))
    
    # å…è®¸çš„éä¸­æ–‡å­—ç¬¦ï¼šæ ‡ç‚¹ç¬¦å·ã€æ•°å­—ã€ç©ºæ ¼ã€ç‰¹æ®Šç¬¦å·
    allowed_non_chinese = r'[\s\d\[\]()ï¼ˆï¼‰ã€‚ï¼Œã€ï¼›ï¼šï¼ï¼Ÿ""''â€¦â€”Â·\-_=\.\,\;\:\!\?\(\)ã€Šã€‹ã€ã€‘]'
    
    # ç§»é™¤å…è®¸çš„éä¸­æ–‡å­—ç¬¦åï¼Œæ£€æŸ¥å‰©ä½™å­—ç¬¦
    remaining_text = re.sub(allowed_non_chinese, '', text)
    non_chinese_chars = sum(1 for char in remaining_text if not is_chinese_char(char))
    
    # å¦‚æœä¸­æ–‡å­—ç¬¦å°‘äº5ä¸ªï¼Œä¸ç®—çº¯ä¸­æ–‡
    if chinese_chars < 5:
        return False
    
    # å¦‚æœéä¸­æ–‡å­—æ¯è¶…è¿‡10%ï¼Œä¸ç®—çº¯ä¸­æ–‡
    total_chars = len(text)
    if non_chinese_chars > total_chars * 0.1:
        return False
    
    # è¿‡æ»¤æ‰æ˜æ˜¾çš„è‹±æ–‡å†…å®¹
    english_patterns = [
        r'[a-zA-Z]{3,}',  # è¿ç»­è‹±æ–‡å­—æ¯
        r'\b(the|and|of|to|in|for|with|by|from|at|on)\b',  # å¸¸è§è‹±æ–‡å•è¯
        r'http[s]?://',  # URL
        r'www\.',  # ç½‘å€
        r'\.com',  # åŸŸå
    ]
    
    for pattern in english_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return False
    
    return True

def extract_pure_chinese_from_html(html_file_path):
    """ä»HTMLæ–‡ä»¶æå–çº¯ä¸­æ–‡å†…å®¹"""
    try:
        with open(html_file_path, 'r', encoding='utf-8') as file:
            content = file.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {html_file_path}: {e}")
        return []
    
    try:
        soup = BeautifulSoup(content, 'html.parser')
        
        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
        for script in soup(["script", "style"]):
            script.decompose()
        
        pure_chinese_texts = []
        
        # æå–æ ‡é¢˜ä¸­çš„ä¸­æ–‡éƒ¨åˆ†
        title = soup.find('title')
        if title:
            title_text = title.get_text().strip()
            # åˆ†å‰²æ ‡é¢˜ï¼Œåªå–ä¸­æ–‡éƒ¨åˆ†
            if '---' in title_text:
                chinese_part = title_text.split('---')[0].strip()
            elif ' - ' in title_text:
                parts = title_text.split(' - ')
                chinese_part = parts[0].strip()
            else:
                chinese_part = title_text
            
            if is_pure_chinese_text(chinese_part):
                pure_chinese_texts.append(f"ã€æ ‡é¢˜ã€‘{chinese_part}")
        
        # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        content_selectors = [
            '#mw-content-text',
            '.mw-parser-output',
            'body',
        ]
        
        main_content = None
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        if not main_content:
            main_content = soup
        
        # æå–å„ç§å…ƒç´ çš„æ–‡æœ¬
        for element in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'td', 'th', 'div', 'span']):
            text = element.get_text().strip()
            
            # å¦‚æœåŒ…å«è‹±æ–‡å’Œä¸­æ–‡æ··åˆï¼Œå°è¯•åˆ†ç¦»
            if text and any(is_chinese_char(c) for c in text):
                # æŒ‰å¥å·æˆ–æ¢è¡Œåˆ†å‰²
                sentences = re.split(r'[ã€‚\n]', text)
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if is_pure_chinese_text(sentence):
                        # æ¸…ç†å¥å­
                        sentence = re.sub(r'\s+', ' ', sentence)  # å‹ç¼©ç©ºæ ¼
                        sentence = re.sub(r'^\d+\.\s*', '', sentence)  # ç§»é™¤å¼€å¤´æ•°å­—
                        sentence = re.sub(r'\[\d+\]', '', sentence)  # ç§»é™¤æ‰€æœ‰å¼•ç”¨æ ‡è®° [1] [2] ç­‰
                        sentence = re.sub(r'\[citation needed\]', '', sentence)  # ç§»é™¤citation needed
                        sentence = re.sub(r'\[æ¥æºè¯·æ±‚\]', '', sentence)  # ç§»é™¤ä¸­æ–‡æ¥æºè¯·æ±‚
                        sentence = re.sub(r'\[éœ€è¦å¼•ç”¨\]', '', sentence)  # ç§»é™¤éœ€è¦å¼•ç”¨æ ‡è®°
                        sentence = re.sub(r'\[ç¼–è¾‘\]', '', sentence)  # ç§»é™¤ç¼–è¾‘æ ‡è®°
                        sentence = re.sub(r'\[edit\]', '', sentence)  # ç§»é™¤è‹±æ–‡ç¼–è¾‘æ ‡è®°
                        sentence = sentence.strip()
                        
                        if len(sentence) > 5:  # åªè¦æœ‰æ„ä¹‰çš„å¥å­
                            pure_chinese_texts.append(sentence)
        
        # å»é‡å¹¶è¿‡æ»¤
        seen = set()
        filtered_texts = []
        for text in pure_chinese_texts:
            if text not in seen and len(text) > 5:
                seen.add(text)
                filtered_texts.append(text)
        
        return filtered_texts
        
    except Exception as e:
        print(f"âŒ è§£æHTMLå¤±è´¥ {html_file_path}: {e}")
        return []

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æå–5ä¸ªå¿ƒç†å­¦è¯æ¡çš„çº¯ä¸­æ–‡å†…å®¹...")
    
    # HTMLæ–‡ä»¶åˆ—è¡¨
    html_files = [
        "resource/DARVO - ç»´åŸºç™¾ç§‘ ---- Wikipedia (2025_9_21 18ï¼š07ï¼š57).html",
        "resource/ä¹ å¾—æ€§å¤±åŠ© - ç»´åŸºç™¾ç§‘ï¼Œè‡ªç”±çš„ç™¾ç§‘å…¨ä¹¦ (2025_9_21 18ï¼š00ï¼š32).html", 
        "resource/åˆ›ä¼¤æ€§ç»“åˆ - ç»´åŸºç™¾ç§‘ --- Traumatic bonding - Wikipedia (2025_9_21 18ï¼š06ï¼š55).html",
        "resource/ç…¤æ°”ç¯æ•ˆåº” - ç»´åŸºç™¾ç§‘ï¼Œè‡ªç”±çš„ç™¾ç§‘å…¨ä¹¦ (2025_9_21 18ï¼š07ï¼š39).html",
        "resource/çˆ±æƒ…è½°ç‚¸ - ç»´åŸºç™¾ç§‘ --- Love bombing - Wikipedia (2025_9_21 18ï¼š06ï¼š27).html"
    ]
    
    # æ¦‚å¿µåç§°æ˜ å°„
    concept_names = {
        "DARVO": "DARVO",
        "ä¹ å¾—æ€§å¤±åŠ©": "ä¹ å¾—æ€§å¤±åŠ©",
        "åˆ›ä¼¤æ€§ç»“åˆ": "åˆ›ä¼¤æ€§ç»“åˆ", 
        "ç…¤æ°”ç¯æ•ˆåº”": "ç…¤æ°”ç¯æ•ˆåº”",
        "çˆ±æƒ…è½°ç‚¸": "çˆ±æƒ…è½°ç‚¸"
    }
    
    all_chinese_content = []
    all_chinese_content.append("=" * 80)
    all_chinese_content.append("å¿ƒç†å­¦æ¦‚å¿µçº¯ä¸­æ–‡å†…å®¹æå–ç»“æœ")
    all_chinese_content.append("=" * 80)
    all_chinese_content.append(f"æå–æ—¶é—´: {os.popen('date').read().strip()}")
    all_chinese_content.append(f"æ€»æ–‡ä»¶æ•°: {len(html_files)}")
    all_chinese_content.append("=" * 80)
    all_chinese_content.append("")
    
    total_extracted = 0
    
    for html_file in html_files:
        if not os.path.exists(html_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {html_file}")
            continue
            
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {os.path.basename(html_file)}")
        
        # ç¡®å®šæ¦‚å¿µåç§°
        concept_name = "æœªçŸ¥æ¦‚å¿µ"
        for key, value in concept_names.items():
            if key in html_file:
                concept_name = value
                break
        
        chinese_texts = extract_pure_chinese_from_html(html_file)
        
        if chinese_texts:
            all_chinese_content.append("=" * 60)
            all_chinese_content.append(f"ğŸ“š {concept_name}")
            all_chinese_content.append("=" * 60)
            all_chinese_content.append(f"æ–‡ä»¶: {os.path.basename(html_file)}")
            all_chinese_content.append(f"çº¯ä¸­æ–‡æ¡ç›®æ•°: {len(chinese_texts)}")
            all_chinese_content.append("-" * 40)
            all_chinese_content.append("")
            
            for i, text in enumerate(chinese_texts, 1):
                all_chinese_content.append(f"{i:03d}. {text}")
                all_chinese_content.append("")
            
            total_extracted += len(chinese_texts)
            print(f"âœ… æˆåŠŸæå– {len(chinese_texts)} æ¡çº¯ä¸­æ–‡å†…å®¹")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°çº¯ä¸­æ–‡å†…å®¹")
    
    # å†™å…¥æ–‡ä»¶
    output_file = "psychology_concepts_pure_chinese.txt"
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(all_chinese_content))
        
        print(f"\nğŸ‰ çº¯ä¸­æ–‡æå–å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“Š æ€»çº¯ä¸­æ–‡æ¡ç›®: {total_extracted}")
        print(f"ğŸ“„ æ€»å†…å®¹è¡Œæ•°: {len(all_chinese_content)}")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        file_size = os.path.getsize(output_file)
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚ ({file_size/1024:.1f} KB)")
        
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    main()